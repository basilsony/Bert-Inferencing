{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Packages\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    print('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Dataset\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Loading Data...........\")\n",
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(\n",
    "    r\"test.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_______________________________________________________________\")\n",
    "print(\"Data Pre-processing...........\")\n",
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Removing the @\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Removing the URL links\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Keeping only letters\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Removing additional whitespaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_______________________________________________________________\")\n",
    "print(\"Tokenization and Data Preparation...........\")\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "def encode_sentence(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))\n",
    "data_inputs = [encode_sentence(sentence) for sentence in data_clean]\n",
    "\n",
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(data_inputs)]\n",
    "sorted_all = [(sent_lab[0], sent_lab[1])\n",
    "              for sent_lab in data_with_len if sent_lab[2] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_input_data(sorted_all):\n",
    "    all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                                 output_types=(tf.int32, tf.int32))\n",
    "    BATCH_SIZE = 32\n",
    "    all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "    return all_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Model Building...........\n"
     ]
    }
   ],
   "source": [
    "print(\"_______________________________________________________________\")\n",
    "print(\"Model Building...........\")\n",
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
    "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
    "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
    "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
    "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
    "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30522 # len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            nb_filters=NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes=NB_CLASSES,\n",
    "            dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15a22fe20f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"final_training/cp.ckpt\"\n",
    "Dcnn.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sorted_all\n",
    "def get_test_data(size: int = 1):\n",
    "    \"\"\"Generates a test dataset of the specified size\"\"\" \n",
    "    num_rows = len(X)\n",
    "    test_df = X.copy()\n",
    "\n",
    "    while num_rows < size:\n",
    "        test_df = test_df + test_df\n",
    "        num_rows = len(test_df)\n",
    "\n",
    "    return test_df[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________\n",
      "Inferencing Started...........\n",
      "running inference for 1 sentences..\n",
      "1 ,       median       mean        std_dev  min_time   max_time  quantile_10  \\\n",
      "0  393611.5  495221.31  846624.461136  351657.0  8904595.0     360585.7   \n",
      "\n",
      "   quantile_90  \n",
      "0     482723.5  \n",
      "running inference for 10 sentences..\n",
      "10 ,      median       mean      std_dev  min_time  max_time  quantile_10  \\\n",
      "0  42911.4  44985.775  7163.502763   36278.5   71930.0     38413.71   \n",
      "\n",
      "   quantile_90  \n",
      "0     54580.48  \n",
      "running inference for 100 sentences..\n",
      "100 ,      median       mean      std_dev  min_time  max_time  quantile_10  \\\n",
      "0  7268.22  7564.9387  1141.178707   5997.33  11999.49     6611.049   \n",
      "\n",
      "   quantile_90  \n",
      "0     9037.028  \n",
      "Summary........\n",
      "      median         mean        std_dev   min_time    max_time  quantile_10  \\\n",
      "0  393611.50  495221.3100  846624.461136  351657.00  8904595.00   360585.700   \n",
      "0   42911.40   44985.7750    7163.502763   36278.50    71930.00    38413.710   \n",
      "0    7268.22    7564.9387    1141.178707    5997.33    11999.49     6611.049   \n",
      "\n",
      "   quantile_90  No_of_Observation  \n",
      "0   482723.500                  1  \n",
      "0    54580.480                 10  \n",
      "0     9037.028                100  \n"
     ]
    }
   ],
   "source": [
    "def calculate_stats(time_list):\n",
    "    \"\"\"Calculate mean and standard deviation of a list\"\"\"\n",
    "    time_array = np.array(time_list)\n",
    "\n",
    "    median = np.median(time_array)\n",
    "    mean = np.mean(time_array)\n",
    "    std_dev = np.std(time_array)\n",
    "    max_time = np.amax(time_array)\n",
    "    min_time = np.amin(time_array)\n",
    "    quantile_10 = np.quantile(time_array, 0.1)\n",
    "    quantile_90 = np.quantile(time_array, 0.9)\n",
    "\n",
    "    basic_key = [\"median\",\"mean\",\"std_dev\",\"min_time\",\"max_time\",\"quantile_10\",\"quantile_90\"]\n",
    "    basic_value = [median,mean,std_dev,min_time,max_time,quantile_10,quantile_90]\n",
    "\n",
    "    dict_basic = dict(zip(basic_key, basic_value))\n",
    "    \n",
    "    return pd.DataFrame(dict_basic, index = [0])\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_LOOPS = 100\n",
    "def run_inference(num_observations:int = 1000):\n",
    "    \"\"\"Run xgboost for specified number of observations\"\"\"\n",
    "    # Load data\n",
    "    test_df = get_test_data(num_observations)\n",
    "    data = bert_input_data(test_df)\n",
    "\n",
    "    num_rows = len(test_df)\n",
    "    print(f\"running inference for {num_rows} sentence(s)..\")\n",
    "    run_times = []\n",
    "    inference_times = []\n",
    "    for _ in range(NUM_LOOPS):\n",
    "\n",
    "        start_time = timer()\n",
    "        Dcnn.predict(data)\n",
    "        end_time = timer()\n",
    "\n",
    "        total_time = end_time - start_time\n",
    "        run_times.append(total_time*10e3)\n",
    "\n",
    "        inference_time = total_time*(10e6)/num_rows\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "    print(num_observations, \", \", calculate_stats(inference_times))\n",
    "    return calculate_stats(inference_times)\n",
    "\n",
    "STATS = '#, median, mean, std_dev, min_time, max_time, quantile_10, quantile_90'\n",
    "\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"Inferencing Started...........\")\n",
    "if __name__=='__main__':\n",
    "    ob_ct = 1  # Start with a single observation\n",
    "    logging.info(STATS)\n",
    "    temp_df = pd.DataFrame()\n",
    "    while ob_ct <= 100:\n",
    "        temp = run_inference(ob_ct)\n",
    "        temp[\"No_of_Observation\"] = ob_ct\n",
    "        temp_df = temp_df.append(temp)\n",
    "        ob_ct *= 10\n",
    "    print(\"Summary........\")\n",
    "    print(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
